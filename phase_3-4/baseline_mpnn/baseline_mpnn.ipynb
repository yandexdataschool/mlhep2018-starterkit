{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import tables\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from random import shuffle\n",
    "from IPython.display import clear_output\n",
    "from sklearn import metrics\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "process\n",
    "USE_GPU = True\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' if USE_GPU else ''\n",
    "EPS = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for phase 1 set num_classes=2\n",
    "# for phase 2 set num_classes=4\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'train_3-4.hdf5'\n",
    "test_file = 'test_3-4.hdf5'\n",
    "submission_file = 'submission_simplified_3-4.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from tools.base import plot_3d, hdf5_to_numpy\n",
    "from tools.tools import stretch_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# N -- number of enties to read. Either int or np.inf. In latter case all entries are readed.\n",
    "N = 20\n",
    "X, Y, M, N = hdf5_to_numpy(file=train_file, n=N, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = -1\n",
    "plot_3d(X[k], Y[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of graph dataset\n",
    "\n",
    "\n",
    "![title](img/knn_graph.png)\n",
    "\n",
    "To compute graph on which we are going to do inference we will use K-nearest neighbours graph. This algorithm draws edges from node(which is, in our case, a hit in the detector) to K closest points.\n",
    "\n",
    "Tunable parameter:\n",
    "\n",
    "__n_neighbors__ -- number of neighbours for k-nearest neighbours graph algo(http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighbors_graph.html).\n",
    "\n",
    "### Ideas\n",
    "\n",
    "  * try to play with default params;\n",
    "  * explore different ideas for graph computation: heuristics, radius neighbors graph, etc.;\n",
    "  * different metrics: manhattan, l1, cosine, metric learning(__hot!__), etc.;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 20\n",
    "\n",
    "in_degree_max, out_degree_max = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.simplified_clustering import generate_graph_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_clusters_graph = []\n",
    "for k in tqdm(range(len(X))):\n",
    "    if len(X[k]) == 0:\n",
    "        continue\n",
    "    # construction of graph based on aggregated statistics\n",
    "    X_cluster_graph, in_degree_max_local, out_degree_max_local = generate_graph_dataset(X=X[k], Y=Y[k], M=M[k],\n",
    "                                                                                        n_neighbors=n_neighbors)\n",
    "    in_degree_max = max(in_degree_max_local, in_degree_max)\n",
    "    out_degree_max = max(out_degree_max_local, out_degree_max)\n",
    "    \n",
    "    X_clusters_graph.append(X_cluster_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is __in_degree_max__ and __out_degree_max__?\n",
    "\n",
    "Well, when you are working with tensorflow you have to specify shape of your data(at least number of columns).\n",
    "\n",
    "```\n",
    "shape = (number_of_nodes, out_degree/in_degree)\n",
    "```\n",
    "\n",
    "__max_out_degree__ is fixed and equal __n_neighbors__ in our setting, but __in_degree_max__ could be different across different events. \n",
    "\n",
    "To anticipate it we are padding all events with edges to non-existing node. Latter this should be taken into account in the MPNN-algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree_max, out_degree_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "for X_cluster_graph in X_clusters_graph:\n",
    "    X_cluster_graph['X_cluster_messages_out'] = stretch_array(X_cluster_graph['X_cluster_messages_out'], \n",
    "                                                              n=out_degree_max, \n",
    "                                                              fill_value=len(X_cluster_graph['X_cluster_edges']))\n",
    "    \n",
    "    X_cluster_graph['X_cluster_messages_in'] = stretch_array(X_cluster_graph['X_cluster_messages_in'], \n",
    "                                                              n=in_degree_max, \n",
    "                                                              fill_value=len(X_cluster_graph['X_cluster_edges']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning model (MPNN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda, GRUCell, GRU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.activations import relu\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster_graph['X_cluster_nodes'].shape, X_cluster_graph['X_cluster_edges'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__X_nodes__ -- features per hit(i.e. energy).\n",
    "\n",
    "__X_edges__ -- features for each edge that connects two hits(i.e. relative difference of coordinates).\n",
    "\n",
    "__X_labels__ -- labels ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim_features_nodes = 4\n",
    "ndim_features_edges = 5\n",
    "ndim_message = 6\n",
    "\n",
    "X_nodes = K.placeholder(shape=(None, ndim_features_nodes)) # features of nodes\n",
    "X_edges = K.placeholder(shape=(None, ndim_features_edges)) # features of edges\n",
    "X_labels = K.placeholder(shape=(None, num_classes)) # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__X_nodes_in_out__ -- edge list.\n",
    "\n",
    "__X_messages_in__ -- in-adjacency lists.\n",
    "\n",
    "__X_messages_out__ -- out-adjacency lists.\n",
    "\n",
    "All these graph representations are equivalent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nodes_in_out = K.placeholder(shape=(None, 2), dtype=np.int32) # edges\n",
    "X_messages_in = K.placeholder(shape=(None, in_degree_max), dtype=np.int32) # shape = (none, size of neighbourhood)\n",
    "X_messages_out = K.placeholder(shape=(None, out_degree_max), dtype=np.int32) # shape = (none, size of neighbourhood)\n",
    "\n",
    "# fake messages to(or from) non-existing node\n",
    "fake_message_const = K.constant(value=[ndim_message * [-np.inf]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholders = {\n",
    "    'X_nodes': X_nodes,\n",
    "    'X_edges': X_edges,\n",
    "    'X_labels': X_labels,\n",
    "    'X_nodes_in_out': X_nodes_in_out,\n",
    "    'X_messages_in': X_messages_in,\n",
    "    'X_messages_out': X_messages_out\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 3\n",
    "\n",
    "message_passers = {\n",
    "    0: Sequential(layers=[\n",
    "                      Dense(16, input_shape=(2 * ndim_features_nodes + ndim_features_edges,), activation=relu), \n",
    "                      Dropout(rate=0.05),\n",
    "                      Dense(ndim_message, activation=relu),\n",
    "                      Dropout(rate=0.05),\n",
    "                  ]\n",
    "                 ),\n",
    "    1: Sequential(layers=[\n",
    "                      Dense(16, input_shape=(2 * ndim_features_nodes + ndim_features_edges,), activation=relu), \n",
    "                      Dropout(rate=0.05),\n",
    "                      Dense(ndim_message, activation=relu),\n",
    "                      Dropout(rate=0.05),\n",
    "                  ]\n",
    "                 ),    \n",
    "    2: Sequential(layers=[\n",
    "                      Dense(16, input_shape=(2 * ndim_features_nodes + ndim_features_edges,), activation=relu), \n",
    "                      Dropout(rate=0.05),\n",
    "                      Dense(ndim_message, activation=relu),\n",
    "                      Dropout(rate=0.05),\n",
    "                  ]\n",
    "                 )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_updater = tf.contrib.rnn.GRUCell(num_units=ndim_features_nodes, )\n",
    "state_updater = Sequential(layers=[\n",
    "                      Dense(16, input_shape=(2 * ndim_message + ndim_features_nodes,), activation=relu), \n",
    "                      Dense(ndim_features_nodes),\n",
    "                                  ]\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readout = Dense(num_classes, input_shape=(ndim_features_nodes,), activation=keras.activations.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPNN construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief explanation of MPNN algorithm in a diagram for a following toy graph:\n",
    "\n",
    "![](img/example_graph.png)\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "![](img/mpnn.png)\n",
    "\n",
    "\n",
    "And corresponding code with comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(X_nodes, X_edges, X_nodes_in_out, \n",
    "                  X_messages_in, X_messages_out, message_passers, \n",
    "                  state_updater, readout, ndim_features_nodes, fake_message_const, steps):\n",
    "    # nodes 'talks' to each other several times which is defined by __step__ parameter\n",
    "    for step in range(steps):\n",
    "        # messages from node to node\n",
    "        messages = message_passers[step](\n",
    "            K.concatenate(\n",
    "                [\n",
    "                    K.reshape(K.gather(reference=X_nodes, indices=X_nodes_in_out), \n",
    "                              shape=(-1, 2 * ndim_features_nodes)), \n",
    "                    X_edges\n",
    "                ], axis=1\n",
    "            )\n",
    "        )\n",
    "        # correct dealing with non-existing edge\n",
    "        messages = K.concatenate([messages, fake_message_const], axis=0)\n",
    "        messages = tf.where(tf.is_inf(messages), tf.zeros_like(messages), messages)\n",
    "\n",
    "        # aggregating messages that came into the node\n",
    "        messages_aggregated_in = K.max(K.gather(reference=messages, indices=X_messages_in), axis=1)\n",
    "        # ... and those exiting node\n",
    "        messages_aggregated_out = K.max(K.gather(reference=messages, indices=X_messages_out), axis=1)\n",
    "\n",
    "        # update nodes states based on messages and previous state\n",
    "        X_nodes = state_updater(K.concatenate([messages_aggregated_in, messages_aggregated_out, X_nodes], axis=1))\n",
    "\n",
    "    return readout(X_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.mpnn import build_network, run_train, run_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predictions = build_network(X_nodes=X_nodes, \n",
    "                              X_edges=X_edges, \n",
    "                              X_nodes_in_out=X_nodes_in_out, \n",
    "                              X_messages_in=X_messages_in, \n",
    "                              X_messages_out=X_messages_out, \n",
    "                              message_passers=message_passers, \n",
    "                              state_updater=state_updater, \n",
    "                              readout=readout, \n",
    "                              steps=steps, \n",
    "                              ndim_features_nodes=ndim_features_nodes,\n",
    "                              fake_message_const=fake_message_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tf = tf.reduce_mean(keras.losses.categorical_crossentropy(X_labels, X_predictions))\n",
    "accuracy_tf = tf.reduce_mean(keras.metrics.categorical_accuracy(X_labels, X_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss_tf, var_list=tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "init.run(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(len(X_clusters_graph) * 0.8)\n",
    "print(TRAIN_SIZE)\n",
    "shuffle(X_clusters_graph)\n",
    "\n",
    "X_clusters_graph_train = X_clusters_graph[:TRAIN_SIZE]\n",
    "X_clusters_graph_eval = X_clusters_graph[TRAIN_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "roc_aucs = []\n",
    "\n",
    "for epoch in tqdm(range(10)):\n",
    "    loss_float = 0\n",
    "    accuracy_float = 0\n",
    "    \n",
    "    losses_epoch = []\n",
    "    accuracies_epoch = []\n",
    "    roc_aucs_epoch = []\n",
    "    for X_cluster_graph in X_clusters_graph_train:\n",
    "        predictions, (loss, accuracy) = run_train(X_cluster_graph=X_cluster_graph,\n",
    "                                   X_predictions=X_predictions,\n",
    "                                   optimizer=optimizer, sess=sess, \n",
    "                                   ndim_features_nodes=ndim_features_nodes, \n",
    "                                   ndim_features_edges=ndim_features_edges, \n",
    "                                   placeholders=placeholders,\n",
    "                                   metrics=[loss_tf, accuracy_tf])\n",
    "        losses_epoch.append(loss)\n",
    "        accuracies_epoch.append(accuracy)\n",
    "    clear_output()\n",
    "    \n",
    "    losses.append(np.mean(losses_epoch))\n",
    "    plt.title('log-loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "\n",
    "    accuracies.append(np.mean(accuracies_epoch))\n",
    "    plt.title('accuracy')\n",
    "    plt.plot(accuracies)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_test = []\n",
    "accuracies_test = []\n",
    "roc_aucs_test = []\n",
    "predictions_total = [] \n",
    "y_total =[]\n",
    "\n",
    "for X_cluster_graph in X_clusters_graph_eval:\n",
    "    predictions, (loss, accuracy) = run_test(X_cluster_graph=X_cluster_graph, \n",
    "                                              X_predictions=X_predictions,\n",
    "                                              sess=sess, \n",
    "                                              ndim_features_nodes=ndim_features_nodes, \n",
    "                                              ndim_features_edges=ndim_features_edges, \n",
    "                                              placeholders=placeholders,\n",
    "                                              metrics=[loss_tf, accuracy_tf])\n",
    "    X_cluster_graph['predictions'] = predictions\n",
    "    predictions_total.append(predictions)\n",
    "    y_total.append(X_cluster_graph['Y_cluster_labels'])\n",
    "    losses_test.append(loss)\n",
    "    accuracies_test.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_total = np.concatenate(predictions_total)\n",
    "y_total = np.concatenate(y_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "accuracy = metrics.accuracy_score(np.argmax(y_total, axis=1), np.argmax(predictions_total, axis=1))\n",
    "roc_auc = metrics.roc_auc_score(y_total, predictions_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test, Y_test, M_test, N_test = hdf5_to_numpy(file=test_file, n=np.inf, num_classes=num_classes, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_clusters_graph_test = []\n",
    "for k in tqdm(range(len(X_test))):\n",
    "    if len(X_test[k]) == 0:\n",
    "        X_cluster_graph = {}\n",
    "        X_cluster_graph['empty'] = True\n",
    "        X_clusters_graph_test.append(X_cluster_graph)\n",
    "        continue\n",
    "        \n",
    "    X_cluster_graph, in_degree_max_local, out_degree_max_local = generate_graph_dataset(X=X_test[k], \n",
    "                                                                                        Y=Y_test[k], \n",
    "                                                                                        M=M_test[k],\n",
    "                                                                                        n_neighbors=n_neighbors, \n",
    "                                                                                        in_degree_max=in_degree_max, \n",
    "                                                                                        out_degree_max=out_degree_max)\n",
    "\n",
    "    X_cluster_graph['empty'] = False\n",
    "    X_clusters_graph_test.append(X_cluster_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_submission(X_cluster_graph):\n",
    "    submission_answer = np.zeros((192, 192, 192))\n",
    "    shift = 0 if num_classes==4 else 1\n",
    "    if X_cluster_graph['empty']:\n",
    "        pass\n",
    "    else:\n",
    "        submission_answer[X_cluster_graph['M'][:, 0], X_cluster_graph['M'][:, 1], X_cluster_graph['M'][:, 2]] = np.argmax(X_cluster_graph['predictions'], axis=1) + shift\n",
    "    X_cluster_graph['submission_answer'] = submission_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_cluster_graph in X_clusters_graph_test:\n",
    "    if not X_cluster_graph['empty']:\n",
    "        predictions, (loss, accuracy) = run_test(X_cluster_graph=X_cluster_graph, \n",
    "                                                  X_predictions=X_predictions,\n",
    "                                                  sess=sess, \n",
    "                                                  ndim_features_nodes=ndim_features_nodes, \n",
    "                                                  ndim_features_edges=ndim_features_edges, \n",
    "                                                  placeholders=placeholders,\n",
    "                                                  metrics=[loss_tf, accuracy_tf])\n",
    "        X_cluster_graph['predictions'] = predictions\n",
    "    \n",
    "    prepare_data_for_submission(X_cluster_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "expectedrows = len(X_test)\n",
    "FILTERS = tables.Filters(complevel=5, complib='zlib', shuffle=True, bitshuffle=False, fletcher32=False, least_significant_digit=None)\n",
    "f_submission = tables.open_file(submission_file, 'w', filters=FILTERS)\n",
    "preds_array = f_submission.create_earray('/', 'pred', tables.UInt32Atom(), (0,192,192,192), expectedrows=expectedrows)\n",
    "\n",
    "for i in tqdm(range(expectedrows)):\n",
    "    preds_array.append(np.expand_dims(X_clusters_graph_test[i]['submission_answer'], axis=0))\n",
    "\n",
    "preds_array.close()\n",
    "f_submission.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final tips & tricks\n",
    "\n",
    "If you want to drive up your score try following things:\n",
    "\n",
    "  * stack MOAR layer;\n",
    "  * more epochs;\n",
    "  * change Dense `state_updater` on LSTM/GRU `state_updater`(btw, you just need to uncomment a bit of code in the section `NNs` and in `tools.mpnn.build_network`);\n",
    "  * data augmentation;\n",
    "  * btw, you might have noticed that I use different `message_passers` but single `state_updater` for each step. This is called _weight tightening_ and used to deal with overfitting;\n",
    "  \n",
    "  You can apply the same technique to `message_passers` or, alternatively, unravel `state_updater`. It's all up to you!\n",
    "  \n",
    "  * do not discard domain knowledge. Even if it's not applicable to feature engeneering you still can use in graph construction / smart clustering / loss function choice / structure of MPNN.\n",
    "  * this version of MPNN learns on one sample per iteration, using batches could impove quality;\n",
    "  * play with learning rate / optimizer type;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
